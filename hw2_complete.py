# -*- coding: utf-8 -*-
"""hw2_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19JAZUtF9-Mbef5dsPAYjcSgElrcu1EKM
"""

from keras.models import Sequential
from keras.models import Model
from keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Add
import keras
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from sklearn.model_selection import train_test_split
import numpy as np
from keras.preprocessing import image




### Add lines to import modules as needed

##

#build model 1
def build_model1():

  model = Sequential()
  # Convolutional layers with BatchNormalization
  model.add(Conv2D(32, (3, 3), strides=(2, 2), padding='same', input_shape=(32, 32, 3)))
  model.add(BatchNormalization())
  model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
  model.add(BatchNormalization())

  # Four more pairs of Conv2D + BatchNormalization
  for _ in range(4):
    model.add(Conv2D(128, (3, 3), padding='same'))
    model.add(BatchNormalization())

  # Last Conv2D + BatchNormalization
  model.add(Conv2D(128, (3, 3), padding='same'))
  model.add(BatchNormalization())

  # MaxPooling layer
  model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4)))

  # Flatten layer
  model.add(Flatten())

  # Fully Connected layers with BatchNormalization
  model.add(Dense(128))
  model.add(BatchNormalization())
  model.add(Dense(10, activation='softmax'))  # Output layer with 10 units for CIFAR-10 classes

  # Compile the model
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
  return model

#build model 2
def build_model2():
  model = Sequential()

  # First Conv2D layer (not replaced)
  model.add(Conv2D(32, (3, 3), strides=(2, 2), padding='same', input_shape=(32, 32, 3)))
  model.add(BatchNormalization())

  # Depthwise-separable convolutional layers with BatchNormalization
  model.add(DepthwiseConv2D((3, 3), strides=(2, 2), padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(64, (1, 1), padding='same'))
  model.add(BatchNormalization())

  model.add(DepthwiseConv2D((3, 3), strides=(2, 2), padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(128, (1, 1), padding='same'))
  model.add(BatchNormalization())

  for _ in range(4):
      model.add(DepthwiseConv2D((3, 3), padding='same'))
      model.add(BatchNormalization())
      model.add(Conv2D(128, (1, 1), padding='same'))
      model.add(BatchNormalization())

  model.add(DepthwiseConv2D((3, 3), padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(128, (1, 1), padding='same'))
  model.add(BatchNormalization())

  # MaxPooling layer
  model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4)))

  # Flatten layer
  model.add(Flatten())

  # Fully Connected layers with BatchNormalization
  model.add(Dense(128))
  model.add(BatchNormalization())
  model.add(Dense(10, activation='softmax'))  # Output layer with 10 units for CIFAR-10 classes

  # Compile the model
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  return model

def build_model3():
    # Input layer
    inputs = Input(shape=(32, 32, 3))
    x = inputs

    # First Convolutional layer with Dropout
    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = Dropout(0.2)(x)  # Dropout after the first convolution

    # Define residual connection function
    def residual_block(input_layer, filters):
        y = Conv2D(filters, (3, 3), padding='same', activation='relu')(input_layer)
        y = BatchNormalization()(y)
        y = Dropout(0.2)(y)
        y = Conv2D(filters, (3, 3), padding='same', activation='relu')(y)
        y = BatchNormalization()(y)
        y = Dropout(0.2)(y)
        if input_layer.shape[-1] != filters:
            input_layer = Conv2D(filters, (1, 1), padding='same')(input_layer)
        return Add()([input_layer, y])

    # Build model architecture with residual connections
    x = residual_block(x, 64)
    x = residual_block(x, 64)
    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)

    x = residual_block(x, 128)
    x = residual_block(x, 128)
    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)

    x = residual_block(x, 128)
    x = residual_block(x, 128)
    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)

    # Flatten layer
    x = Flatten()(x)

    # Fully Connected layers with Dropout
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(10, activation='softmax')(x)  # Output layer with 10 units for CIFAR-10 classes

    # Create model
    model3 = Model(inputs=inputs, outputs=outputs)

    return model3



def build_model50k():
    model = None # Add code to define model 1.
    return model






# no training or dataset construction should happen above this line
if __name__ == '__main__':

  ########################################
  ## Add code here to Load the CIFAR10 data set

  (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

  # Normalize pixel values to range [0, 1]
  train_images = train_images / 255.0
  test_images = test_images / 255.0

  # Convert labels to one-hot encoded format
  num_classes = 10
  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)
  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)


  # Split training set into training and validation subsets
  train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)


  ########################################
  ## Build and train model 1
  model1 = build_model1()
  #MODEL SUMMARY
   model1.summary()

  # compile and train model 1.
  model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  # Train the model for 50 epochs
  history = model1.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

  # Evaluate the model on test set
  test_loss, test_accuracy = model1.evaluate(test_images, test_labels)

  def preprocess_image(image_path):
    img = image.load_img(image_path, target_size=(32, 32))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalize pixel values
    return img_array

  def classify_image(model, image_path):
    img_array = preprocess_image(image_path)
    prediction = model.predict(img_array)
    return prediction

  # Example usage:
  import sys
  image_path = r'C:/Users/nerdmaidz/OneDrive/Desktop/UNCC/test_image_bird.png'
  prediction = classify_image(model1, image_path)
  print("Prediction:", prediction)

  # Get training and validation accuracies
  training_accuracy = history.history['accuracy'][-1]
  validation_accuracy = history.history['val_accuracy'][-1]

  print("Training Accuracy:", training_accuracy)
  print("Validation Accuracy:", validation_accuracy)
  print("Test Accuracy:", test_accuracy)


  ## Build, compile, and train model 2 (DS Convolutions)
  model2 = build_model2()
  #MODEL SUMMARY
  model2.summary()

  # compile and train model 2.
  model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  # Train the model for 50 epochs
  history = model2.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

  # Evaluate the model on test set
  test_loss, test_accuracy = model2.evaluate(test_images, test_labels)

  # Get training and validation accuracies
  training_accuracy = history.history['accuracy'][-1]
  validation_accuracy = history.history['val_accuracy'][-1]

  print("Training Accuracy:", training_accuracy)
  print("Validation Accuracy:", validation_accuracy)
  print("Test Accuracy:", test_accuracy)


  ### Repeat for model 3

# Build model3
model3 = build_model3()

# Display model summary
model3.summary()

# compile and train model 3.
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  # Train the model for 50 epochs
history = model3.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

  # Evaluate the model on test set
test_loss, test_accuracy = model3.evaluate(test_images, test_labels)

  # Get training and validation accuracies
training_accuracy = history.history['accuracy'][-1]
validation_accuracy = history.history['val_accuracy'][-1]

print("Training Accuracy:", training_accuracy)
print("Validation Accuracy:", validation_accuracy)
print("Test Accuracy:", test_accuracy)